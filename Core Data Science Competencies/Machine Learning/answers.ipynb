{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f376374",
   "metadata": {},
   "source": [
    "### Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157d074",
   "metadata": {},
   "source": [
    "D. The model is likely overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdca36c",
   "metadata": {},
   "source": [
    "High accuracy with low recall implies that the model is doing well overall because most predictions belong to the majority class, but it’s failing to correctly identify positive (minority) cases.\n",
    "This typically indicates class imbalance — the model predicts the majority class more frequently, inflating accuracy but harming recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e37a30",
   "metadata": {},
   "source": [
    "In such cases, you’d consider recall-oriented metrics (F1-score, ROC-AUC) and apply resampling techniques or class-weight adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167828c",
   "metadata": {},
   "source": [
    "### Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc6b15",
   "metadata": {},
   "source": [
    "B. L1 can shrink some coefficients to zero, enabling feature selection, while L2 cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ecd5d",
   "metadata": {},
   "source": [
    "L1 (Lasso) regularization adds a penalty proportional to the absolute value of the coefficients and can shrink some to zero, effectively performing feature selection.\n",
    "L2 (Ridge) regularization adds a penalty proportional to the square of the coefficients, shrinking them toward zero but never completely eliminating any."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdbf03",
   "metadata": {},
   "source": [
    "- L1 → promotes sparsity (feature selection).\n",
    "- L2 → promotes smooth shrinkage (handles multicollinearity).\n",
    "- Regularization in both cases aims to reduce overfitting, not to “increase bias” arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ac6e1",
   "metadata": {},
   "source": [
    "### Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f28f0d",
   "metadata": {},
   "source": [
    "B. Boosting corrects previous models’ errors iteratively, while bagging reduces variance through random resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4899d0",
   "metadata": {},
   "source": [
    "- Bagging (Bootstrap Aggregating) → builds models in parallel on random subsets to reduce variance.\n",
    "- Boosting → builds models sequentially, each new model correcting the errors of the previous one, reducing bias and improving performance iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8d416",
   "metadata": {},
   "source": [
    "- Boosting = sequential error correction.\n",
    "- Bagging = parallel variance reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c84581",
   "metadata": {},
   "source": [
    "### Q4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7323e77",
   "metadata": {},
   "source": [
    "C. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e43db",
   "metadata": {},
   "source": [
    "The curse of dimensionality most strongly affects distance-based algorithms, such as K-Nearest Neighbors (KNN), because the notion of distance becomes less meaningful as the number of features increases.\n",
    "Random Forests and Decision Trees are relatively robust to high dimensions due to feature sampling and splitting criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5801c",
   "metadata": {},
   "source": [
    "High-dimensional data can lead to sparse distributions, making Euclidean distance metrics unreliable. Dimensionality reduction (e.g., PCA, feature selection) becomes essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316a563",
   "metadata": {},
   "source": [
    "### Q5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df4732",
   "metadata": {},
   "source": [
    "B. Increasing the number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f50ed8",
   "metadata": {},
   "source": [
    "If test performance drops while training performance is high → overfitting.\n",
    "Adding more hidden layers typically increases model capacity, which may worsen overfitting rather than mitigate it.\n",
    "Techniques like dropout, early stopping, and data augmentation are designed to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0af83",
   "metadata": {},
   "source": [
    "Always balance model complexity with regularization and generalization strategies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
