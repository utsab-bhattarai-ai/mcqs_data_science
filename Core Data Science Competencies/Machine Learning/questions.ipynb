{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0392d51",
   "metadata": {},
   "source": [
    "### Q1. In a binary classification problem, the model shows high accuracy but very low recall. What does this indicate about the model’s behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d20800",
   "metadata": {},
   "source": [
    "A. The model performs well on both classes equally.\n",
    "\n",
    "B. The model is biased towards the minority class.\n",
    "\n",
    "C. The model is likely overfitting on the training data.\n",
    "\n",
    "D. The model is predicting the majority class more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ebc82",
   "metadata": {},
   "source": [
    "### Q2. When applying Regularization in linear models, which of the following statements best describes the difference between L1 (Lasso) and L2 (Ridge) regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccbeb4",
   "metadata": {},
   "source": [
    "A. L1 adds a penalty proportional to the square of the coefficients, while L2 adds an absolute penalty.\n",
    "\n",
    "B. L1 can shrink some coefficients to zero, enabling feature selection, while L2 cannot.\n",
    "\n",
    "C. L1 regularization reduces model variance, while L2 increases bias.\n",
    "\n",
    "D. L2 regularization is non-differentiable at zero, while L1 is smooth everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757b7cf",
   "metadata": {},
   "source": [
    "### Q3. In ensemble learning, particularly in bagging and boosting, what is the fundamental difference in their approach to model improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a00253",
   "metadata": {},
   "source": [
    "A. Bagging combines weak models sequentially, while boosting combines them in parallel.\n",
    "\n",
    "B. Boosting corrects previous models’ errors iteratively, while bagging reduces variance through random resampling.\n",
    "\n",
    "C. Both bagging and boosting primarily aim to reduce model bias.\n",
    "\n",
    "D. Bagging always uses decision trees, whereas boosting cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a5af4",
   "metadata": {},
   "source": [
    "### Q4. Consider a dataset with 1000 samples and 5000 features. Which algorithm is most likely to suffer from the curse of dimensionality if no feature selection or dimensionality reduction is performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf6ac2",
   "metadata": {},
   "source": [
    "A. Decision Tree\n",
    "\n",
    "B. Naïve Bayes\n",
    "\n",
    "C. K-Nearest Neighbors (KNN)\n",
    "\n",
    "D. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac630853",
   "metadata": {},
   "source": [
    "### Q5. A data scientist observes that the test set performance of a neural network is significantly worse than the training set performance. Which of the following strategies is least effective in mitigating this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9c729",
   "metadata": {},
   "source": [
    "A. Adding dropout layers during training\n",
    "\n",
    "B. Increasing the number of hidden layers\n",
    "\n",
    "C. Applying early stopping based on validation loss\n",
    "\n",
    "D. Using data augmentation techniques"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
